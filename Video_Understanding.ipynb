{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFyLtr5vRiNN4goNpm/giM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ywchanna2001/AI-ML-Football-Analysis-system/blob/main/Video_Understanding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "wppZmGFEBaBy",
        "outputId": "16b508a3-adf8-48a9-e326-183f49dc40ca"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Mot-JEU26GQ?si=pcb7-_MZTSi_1Zkw\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title Building with Gemini 2.0: Video understanding\n",
        "%%html\n",
        "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Mot-JEU26GQ?si=pcb7-_MZTSi_1Zkw\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install SDK"
      ],
      "metadata": {
        "id": "PwjBzyiyCSdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U -q 'google-genai'"
      ],
      "metadata": {
        "id": "I1BFx_YfB1ET"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup API key"
      ],
      "metadata": {
        "id": "3M_UYv84CVv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "iOiznui4B9nK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize SDK client"
      ],
      "metadata": {
        "id": "ThBJVXuDChfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "BWQMUr1LCFvu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(client))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9GD4qclPySk",
        "outputId": "93a6be2a-a5f5-4f1e-f606-60ea1493f011"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'google.genai.client.Client'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Select the Gemini model\n",
        "\n",
        "Video understanding works best Gemini 2.5 pro model. We can also select former models to compare their behavior but it is recommended to use at least the 2.0 ones.\n",
        "\n",
        "Official comparison:\n",
        "https://deepmind.google/technologies/gemini/pro/"
      ],
      "metadata": {
        "id": "zcmAh4H8DUwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gemini-2.5-pro-exp-03-25\" # @param [\"gemini-1.5-flash-latest\",\"gemini-2.0-flash-lite\",\"gemini-2.0-flash\",\"gemini-2.5-pro-exp-03-25\"] {\"allow-input\":true, isTemplate: true}"
      ],
      "metadata": {
        "id": "uVIg9jPfCkQX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyze youtube videos\n",
        "\n",
        "Until we create a rabies dog video data set we decided to analyze and process the videos that include rabid dogs' visuals."
      ],
      "metadata": {
        "id": "xr66lgpyEY8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from PIL import Image\n",
        "from IPython.display import display, Markdown, HTML\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=model_name,\n",
        "    contents=types.Content(\n",
        "        parts=[\n",
        "            types.Part(text=\"Find all the instances where Sundar says \\\"AI\\\". Provide timestamps and broader context for each instance.\"),\n",
        "            types.Part(\n",
        "                file_data=types.FileData(file_uri='https://www.youtube.com/watch?v=ixRanV-rdAQ')\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zhrFGo4fD6Xv",
        "outputId": "68d282f0-407d-4a57-87ac-bab346db7e4b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, here are all the instances where Sundar Pichai says \"AI\" in the provided video clip, along with timestamps and context:\n\n1.  **0:29** - \"**AI** is having a very busy year.\"\n    *   **Context:** Sundar is opening the keynote, welcoming the audience, and acknowledging the significant recent developments and public interest in AI, setting the stage for the event's focus.\n2.  **0:38** - \"...as an **AI** first company...\"\n    *   **Context:** He's reflecting on Google's journey, stating they've been an \"AI first\" company for seven years and are now at an exciting inflection point.\n3.  **0:45** - \"...opportunity to make **AI** even more helpful...\"\n    *   **Context:** Sundar is outlining Google's goal to leverage the current advancements to make AI more beneficial for everyone (people, businesses, communities).\n4.  **0:54** - \"We've been applying **AI** to make our products radically more helpful...\"\n    *   **Context:** He explains that Google has already been using AI for some time to improve its products, setting up the transition to discuss *generative* AI's impact.\n5.  **1:17** - \"...how generative **AI** is helping to evolve our products...\"\n    *   **Context:** He specifically mentions generative AI as the next step in evolving Google's products and introduces Gmail as the first example.\n6.  **1:40** - \"...more advanced writing features powered by **AI**.\"\n    *   **Context:** Sundar is tracing the evolution of Gmail features from Smart Reply to Smart Compose, noting that the latter led to more sophisticated AI-powered writing tools in Workspace.\n7.  **3:02** - \"**AI** has stitched together billions of panoramic images...\"\n    *   **Context:** Introducing Google Maps as the next example, he explains how AI was fundamental to creating Street View from the beginning.\n8.  **3:13** - \"...which uses **AI** to create a high fidelity representation of a place...\"\n    *   **Context:** He's describing Immersive View in Maps, explaining that AI is used to build the detailed 3D models of locations.\n9.  **5:07** - \"Another product made better by **AI** is Google Photos.\"\n    *   **Context:** Transitioning to the third product example, highlighting AI's role in enhancing Google Photos.\n10. **5:15** - \"...one of our first **AI** native products.\"\n    *   **Context:** Sundar is emphasizing that Google Photos was designed with AI at its core from its launch in 2015.\n11. **5:38** - \"**AI** advancements give us more powerful ways to do this.\"\n    *   **Context:** He's discussing photo editing capabilities in Google Photos and how AI progress enables more advanced features.\n12. **5:47** - \"...uses **AI** powered computational photography...\"\n    *   **Context:** Explaining the technology behind the Magic Eraser feature in Google Photos.\n13. **5:58** - \"...semantic understanding and generative **AI**...\"\n    *   **Context:** Introducing the upcoming Magic Editor feature and mentioning the combination of AI techniques (semantic understanding and generative AI) that power it.\n14. **7:40** - \"...how **AI** can help you in moments that matter.\"\n    *   **Context:** Summarizing the product examples (Gmail, Maps, Photos) just shown, framing them as ways AI provides practical help.\n15. **7:47** - \"...full potential of **AI** across the products you know and love.\"\n    *   **Context:** Stating Google's overarching goal to integrate AI capabilities throughout its product suite.\n16. **8:22** - \"...making **AI** helpful for everyone...\"\n    *   **Context:** He introduces this phrase as the core theme for Google's AI strategy and the most profound way to advance their mission. This phrase appears on the screen.\n17. **8:53** - \"...building and deploying **AI** responsibly...\"\n    *   **Context:** Sundar lists the fourth key way Google is making AI helpful, focusing on responsible development and deployment.\n18. **9:02** - \"...ability to make **AI** helpful for everyone relies on...\"\n    *   **Context:** Connecting the goal of helpful AI directly to the need for advancing Google's underlying foundation models.\n19. **11:26** - \"It uses **AI** to better detect malicious scripts...\"\n    *   **Context:** Describing the capabilities of Sec-PaLM, a specialized version of PaLM 2 fine-tuned for security use cases.\n20. **12:45** - \"...bring **AI** in responsible ways to billions of people.\"\n    *   **Context:** Framing the development of PaLM 2 as the latest step in Google's long-term commitment to responsible AI deployment.\n21. **12:57** - \"...defining **AI** breakthroughs over the last decade...\"\n    *   **Context:** Crediting the Google Brain and DeepMind teams (now merged) for many significant AI advancements.\n22. **14:09** - \"...deeply investing in **AI** responsibility.\"\n    *   **Context:** Introducing the topic of tools for identifying synthetic content as part of Google's commitment to responsible AI.\n23. **15:04** - \"...one of our **AI** generated images has that metadata.\"\n    *   **Context:** Committing to including metadata in all AI-generated images produced by Google's tools to aid identification.\n24. **15:11** - \"...responsible approach to **AI** later.\"\n    *   **Context:** Mentioning that James Manyika will discuss Google's responsible AI approach in more detail later in the keynote.\n25. **15:29** - \"...experiment for conversational **AI**.\"\n    *   **Context:** Describing Google Bard as an experiment in the field of conversational AI."
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.genai as genai\n",
        "from google.genai import types\n",
        "import json # Keep these imports if you need them elsewhere\n",
        "from PIL import Image # Keep these imports if you need them elsewhere\n",
        "from IPython.display import display, Markdown, HTML # Keep these imports if you need them elsewhere\n",
        "\n",
        "# --- Assume these are already defined from your notebook setup ---\n",
        "# from google.colab import userdata\n",
        "# GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "# client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "# model_name = \"gemini-1.5-pro-preview-latest\" # Or your chosen model like \"gemini-2.5-pro-exp-03-25\"\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "def analyze_youtube_video(prompt: str, youtube_url: str, client: genai.Client, model: str) -> str:\n",
        "    \"\"\"\n",
        "    Analyzes a YouTube video using the Gemini API with a custom prompt.\n",
        "\n",
        "    Args:\n",
        "        prompt: The text prompt/instructions for the AI model.\n",
        "        youtube_url: The URL of the YouTube video to analyze.\n",
        "        client: The initialized google.genai.Client instance.\n",
        "        model: The name of the Gemini model to use (e.g., \"gemini-1.5-pro-preview-latest\").\n",
        "\n",
        "    Returns:\n",
        "        The text response generated by the Gemini model, or an error message.\n",
        "\n",
        "    Raises:\n",
        "        # Catches general exceptions, specific API errors might be handled differently.\n",
        "        Exception: If the API call fails for reasons other than generation blocking.\n",
        "    \"\"\"\n",
        "    print(f\"Attempting to analyze YouTube video: {youtube_url}\")\n",
        "    print(f\"Using Model: {model}\")\n",
        "    print(f\"Prompt: '{prompt}'\")\n",
        "\n",
        "    try:\n",
        "        # Create the video part using FileData with the URI\n",
        "        video_part = types.Part(\n",
        "            file_data=types.FileData(file_uri=youtube_url)\n",
        "        )\n",
        "\n",
        "        # Create the text prompt part\n",
        "        prompt_part = types.Part(text=prompt)\n",
        "\n",
        "        # Construct the content object with both parts\n",
        "        # The order of parts generally doesn't matter, but prompt first is common\n",
        "        content_to_send = types.Content(parts=[prompt_part, video_part])\n",
        "\n",
        "        # Make the API call\n",
        "        response = client.models.generate_content( # Use client.generate_content, not client.models...\n",
        "            model=model,\n",
        "            contents=content_to_send\n",
        "        )\n",
        "\n",
        "        # Check if the response has text (it might be blocked)\n",
        "        if hasattr(response, 'text'):\n",
        "             print(\"Analysis successful.\")\n",
        "             return response.text\n",
        "        else:\n",
        "             # Handle cases where the response might be empty or blocked\n",
        "             print(\"Warning: Response did not contain text. It might have been blocked.\")\n",
        "             # You might want to inspect response.prompt_feedback or other attributes\n",
        "             return f\"Analysis generated no text output. Response details: {response}\"\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        # Handle potential errors during the API call\n",
        "        print(f\"An error occurred during Gemini API call: {e}\")\n",
        "        # You might want to log the full exception traceback here\n",
        "        # import traceback\n",
        "        # traceback.print_exc()\n",
        "        return f\"Error analyzing video: {e}\"\n",
        "\n",
        "# --- Example Usage ---\n",
        "\n",
        "# **Important:** Make sure 'client' and 'model_name' are defined and valid before running this.\n",
        "if 'client' in locals() and 'model_name' in locals():\n",
        "\n",
        "    # Define your custom prompt and video URL\n",
        "    my_custom_prompt = \"Describe the main visual elements and the overall mood of this video clip in three sentences.\"\n",
        "    video_to_analyze_url = 'https://www.youtube.com/watch?v=ixRanV-rdAQ' # Google I/O Keynote example\n",
        "\n",
        "    # Call the function\n",
        "    analysis_result = analyze_youtube_video(\n",
        "        prompt=my_custom_prompt,\n",
        "        youtube_url=video_to_analyze_url,\n",
        "        client=client,      # Pass the initialized client\n",
        "        model=model_name    # Pass the desired model name\n",
        "    )\n",
        "\n",
        "    # Display the result using Markdown\n",
        "    print(\"\\n--- Analysis Result ---\")\n",
        "    if analysis_result.startswith(\"Error\"):\n",
        "        print(analysis_result) # Print error messages directly\n",
        "    else:\n",
        "        display(Markdown(analysis_result))\n",
        "\n",
        "else:\n",
        "    print(\"Error: Please ensure 'client' (genai.Client) and 'model_name' are defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "kMxUf9upE48A",
        "outputId": "237a7031-1c3e-485b-abf8-90d86872ce64"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to analyze YouTube video: https://www.youtube.com/watch?v=ixRanV-rdAQ\n",
            "Using Model: gemini-2.5-pro-exp-03-25\n",
            "Prompt: 'Describe the main visual elements and the overall mood of this video clip in three sentences.'\n",
            "Analysis successful.\n",
            "\n",
            "--- Analysis Result ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "This clip features Google CEO Sundar Pichai presenting at the Google I/O conference on a large, modern outdoor stage characterized by warm wood tones, minimalist curves, and accents of Google's signature colors. The primary visuals alternate between close-ups of Pichai and wide shots encompassing the stage, a large central screen displaying various graphics and product demonstrations (like Gmail, Maps, and Photos AI features), and the large, engaged audience. The overall mood is energetic, informative, and forward-looking, highlighting Google's focus on artificial intelligence advancements and product innovation."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VE4nEApEOJ1Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}